# Web Scraping with Python

## Overview and Goal

The idea is to create a code that scrapes data for sneakers (Nike Dunk) from a European clothing online retailer, Zalando, keeps track of prices, continuously stores them in a .csv file and sends an email notification if price of any sneakers is below 800 SEK (Swedish krona). For this the selenium and smtplib packages are used. The code also contains a function which plots how price of specific sneakers changes over time. 

## Code

First, one needs to define an email function (send_email) since it'll be needed later on in the function that scrapes data. For this, the smtplib package is utilized. The main task here is to connect to the server using smtplib.SMTP_SSL by specifying the SMTP and the port of the email that you use. This is something that can be found easily. For gmail.com, the SMTP  is smtp.gmail.com and the port is 465. A warning for those who have a gmail address: in order to send an email to your address, one needs to turn on the 2-step verification and create an App Password (a 16-digit password which is used as 'your password' below). Although, Google says that App Passwords arenâ€™t recommended and are unnecessary in most cases as they are less safe. But this seems to be the only way to make it work. To my knowledge, the issue is non existent on other emails like Yahoo. In order to login to your email account, the command .login('email', 'password') is used. Writing a subject and a message, the email is sent using the .sendmail() command.

The next objective is to write a function (check_price) that scrapes data, stores it in a .csv file and updates it. One option here is to use the selenium package, but one could also choose to procced with the BeautifulSoup package which is also a good alternative. With selenium, one needs to specify the browser that one uses and the location of the corresponding webdriver on computer, which is achieved using the webdriver. commandeline. For instance, in my case the browser is Safari, so the line is webdriver.Safari('/usr/bin/safaridriver'). The next step is to get the url of the Zaladno page and navigate to it with driver.get(url) command. Zalando uses continuously scrolling pages, so I also added a codeline that scrolls down to the end of the page ( driver.execute_script("window.scrollTo(0, document.body.scrollHeight);") ) and waits 2 seconds until the content is loaded. Then, the main part begins - data scraping. This is done using the .find_elements command which finds and extracts brand, colourway and price of all the sneakers on the page by their tag. The only issue that arises here is that there are multiple price tags. I suppose the reason for this is that differnet tags are used for prices with and without discounts. One can either specify all of them or use the general tag, but this will get you current price and price before, which is why the re (regular expression) package to extract all current prices. All elements are then stored in a list as (brand, sneakers, colourway, price). If the any price is lower than 800 SEK, the email function is called. The final step is put everything is a .csv file and save it. This is easily achieved by firstly creating a data frame with the data and then by writing the line .to_csv(). Also, if such file already exists, i.e. if this is not the first time the prices were checked, the file is updated by adding another column with current prices. In the end, one has a file with sneakers and their colourway, and columns with prices and date, which helps to keep track when the prices were extracted. The statement while True and the subsequent 2 codelines are then used to run the function daily. This way, one has an automatic proccess that looks up the prices, stores their histroy and notifies if any pair of sneakers is below 800 SEK. 

Finally, there's a function (price_evolution) that plots history of prices of a specific pair of sneakers. This might be useful since visualisation usually helps to comprehend such information better. Also, here I've chosen not to plot the histories of all the sneakers simultaneously since it becomes quite messy (there are 40+ sneakers). 
